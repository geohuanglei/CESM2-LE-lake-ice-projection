{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this manuscript is to calculate ice phenology from daily lake ice thickness in CESM2-LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # version '1.20.0'\n",
    "import xarray as xr # version '0.16.2'\n",
    "from tqdm import tqdm # version '4.43.0'\n",
    "import matplotlib.pyplot as plt # version '3.3.2'\n",
    "import matplotlib as mpl # version '3.3.2'\n",
    "import cmocean # version '2.0'\n",
    "import cartopy.crs as ccrs # version '0.18.0'\n",
    "import cartopy.feature as cf # version '0.18.0'\n",
    "import glob\n",
    "import dask.array as da # version '2021.1.1'\n",
    "import time\n",
    "from numpy import nan\n",
    "import seaborn as sns # version '0.11.0'\n",
    "import pandas as pd # version 1.2.1'\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://203.247.189.224:43634</li>\n",
       "  <li><b>Dashboard: </b><a href='http://203.247.189.224:8787/status' target='_blank'>http://203.247.189.224:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>16</li>\n",
       "  <li><b>Cores: </b>576</li>\n",
       "  <li><b>Memory: </b>320.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://203.247.189.224:43634' processes=16 threads=576, memory=320.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup the dask parallel computing\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file= '/.../scheduler.json') # the path to the scheduler file\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 define functions to read in nc files across all the ensemble members\n",
    "The variables stored in the directories of individual ensemble members ,we use this function to read the data of all the ensemble members into one xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_process_coords(exceptcv=[]):\n",
    "    def process_coords(ds, except_coord_vars=exceptcv):\n",
    "        coord_vars = []\n",
    "        for v in np.array(ds.coords):\n",
    "            if not v in except_coord_vars:\n",
    "                coord_vars += [v]\n",
    "        for v in np.array(ds.data_vars):\n",
    "            if not v in except_coord_vars:\n",
    "                coord_vars += [v]\n",
    "        return ds.drop(coord_vars)\n",
    "    return process_coords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def read_in(var, exceptcv, domain='lnd/', freq='day_1/', stream='h6', chunks=dict(time=365), ens_s=-20, ens_e=-10):\n",
    "    ens_dir = \"path for the home directory of the whole CESM-LE data\"\n",
    "    histens_names = [member.split('archive/')[1][:-1]\n",
    "                     for member in sorted(glob.glob(ens_dir + \"b.e21.BHIST*LE2*[!old][!tmp]/\"))][10:]\n",
    "    projens_names = [member.split('archive/')[1][:-1] for member in sorted(\n",
    "        glob.glob(ens_dir + \"b.e21.BSSP370*.f09_g17*[!old][!tmp]/\"))][10:]\n",
    "    hist_ncfiles = []\n",
    "    proj_ncfiles = []\n",
    "    for i in np.arange(ens_s, ens_e):\n",
    "        hist_fnames = sorted(glob.glob(\n",
    "            ens_dir + histens_names[i] + \"/\" + domain + \"proc/tseries/\" + freq + \"*\" + stream + var + \"*\"))\n",
    "        proj_fnames = sorted(glob.glob(\n",
    "            ens_dir + projens_names[i] + \"/\" + domain + \"proc/tseries/\" + freq + \"*\" + stream + var + \"*\"))\n",
    "        hist_ncfiles.append(hist_fnames)\n",
    "        proj_ncfiles.append(proj_fnames)\n",
    "    ens_numbers = [members.split('LE2-')[1]\n",
    "                   for members in histens_names][ens_s:ens_e]\n",
    "    hist_ds = xr.open_mfdataset(hist_ncfiles,\n",
    "                                chunks=chunks,\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                                combine='nested',\n",
    "                                concat_dim=[[*ens_numbers], 'time'],\n",
    "                                parallel=True)\n",
    "    proj_ds = xr.open_mfdataset(proj_ncfiles,\n",
    "                                chunks=chunks,\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                                combine='nested',\n",
    "                                concat_dim=[[*ens_numbers], 'time'],\n",
    "                                parallel=True)\n",
    "    if freq == 'day_1/':\n",
    "        hist_ds = hist_ds.isel(time=np.arange(1, hist_ds.time.shape[0]))\n",
    "        proj_ds = proj_ds.isel(time=np.arange(1, proj_ds.time.shape[0]))\n",
    "        hist_ds['time'] = hist_ds.time.get_index('time').shift(-1, 'D')\n",
    "        proj_ds['time'] = proj_ds.time.get_index('time').shift(-1, 'D')\n",
    "    if freq == 'month_1/':\n",
    "        hist_ds['time'] = hist_ds.time.get_index('time').shift(-1, 'M')\n",
    "        proj_ds['time'] = proj_ds.time.get_index('time').shift(-1, 'M')\n",
    "    ens_ds = xr.concat((hist_ds, proj_ds), 'time')\n",
    "    ens_ds = ens_ds.rename({'concat_dim': 'ensemble'})\n",
    "    return ens_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate iceduration, iceon, iceoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 read in daily ice thickness data from CESM2-LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['LAKEICETHICK']\n",
    "exceptcv = ['time', 'lat', 'lon', *variables]\n",
    "ice_ds = read_in(var = '.LAKEICETHICK.', \n",
    "                 exceptcv= exceptcv, \n",
    "                 freq = 'day_1/', \n",
    "                 stream = 'h5',\n",
    "                 chunks =dict(time= 50), \n",
    "                 ens_s = 0, \n",
    "                 ens_e = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 calculate ice phenology from daily lake ice thickness in CESM2-LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in the Northern Hemisphere, one ice cover season spread among two calendar years,\n",
    "to get the right day of year for ice on data, we shift the whole time index by -232 days, so that the ice season is within one calendar year and easy to calculate the ice duraiton, iceon date, ice off date\n",
    "'''\n",
    "daily_ice  = ice_ds.LAKEICETHICK\n",
    "daily_ice['time'] = daily_ice.time.get_index('time').shift(-232,\"D\")\n",
    "daily_ice  = daily_ice.sel(time = slice('1850-01-01','2099-12-31'))\n",
    "years = np.unique(daily_ice.time.dt.year)\n",
    "dayofyear = np.unique(daily_ice.time.dt.dayofyear)\n",
    "ind = pd.MultiIndex.from_product((years,dayofyear),names=('year','dayofyear'))\n",
    "daily_ice_res = daily_ice.assign_coords(time = ind).unstack('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceon_mod = xr.DataArray(nan, \n",
    "                     attrs=daily_ice.attrs,\n",
    "                     name='LAKEICETHICK',\n",
    "                     dims=('ensemble','lat','lon','year'), \n",
    "                     coords = {'ensemble':daily_ice.ensemble,'lat':daily_ice.lat,'lon':daily_ice.lon,'year':np.arange(1850,2100)})\n",
    "iceoff_mod = xr.DataArray(nan, \n",
    "                     attrs=daily_ice.attrs,\n",
    "                     name='LAKEICETHICK',\n",
    "                     dims=('ensemble','lat','lon','year'), \n",
    "                     coords = {'ensemble':daily_ice.ensemble,'lat':daily_ice.lat,'lon':daily_ice.lon,'year':np.arange(1850,2100)})\n",
    "iceduration_mod = xr.DataArray(nan, \n",
    "                     attrs=daily_ice.attrs,\n",
    "                     name='LAKEICETHICK',\n",
    "                     dims=('ensemble','lat','lon','year'), \n",
    "                     coords = {'ensemble':daily_ice.ensemble,'lat':daily_ice.lat,'lon':daily_ice.lon,'year':np.arange(1850,2100)})\n",
    "daily_ice_max = xr.DataArray(nan,\n",
    "                            attrs = daily_ice.attrs,\n",
    "                            name='LAKEICETHICK',\n",
    "                            dims = ('ensemble','lat','lon','year'),\n",
    "                            coords = {'ensemble':daily_ice.ensemble,'lat':daily_ice.lat,'lon':daily_ice.lon, 'year':np.arange(1850,2100)})\n",
    "ice_thick_ann = xr.DataArray(nan,\n",
    "                            attrs = daily_ice.attrs,\n",
    "                            name = 'LAKEICETHICK',\n",
    "                            dims = ('ensemble', 'lat', 'lon', 'year'),\n",
    "                            coords = {'ensemble':daily_ice.ensemble,'lat':daily_ice.lat,'lon':daily_ice.lon, 'year':np.arange(1850,2100)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1st loop: 100%|██████████| 10/10 [07:50<00:00, 47.08s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(np.arange(0,90), desc = '1st loop'):\n",
    "    daily_ice_oneens = daily_ice_res[i,:,:,:,:]\n",
    "    dayofyear_mask = daily_ice_oneens.dayofyear.where(daily_ice_oneens >0)\n",
    "    iceduration_mod[i,:,:,:] = dayofyear_mask.count('dayofyear')\n",
    "    iceon_mod[i,:,:,:]       = dayofyear_mask.min('dayofyear')\n",
    "    iceoff_mod[i,:,:,:]      = dayofyear_mask.max('dayofyear')\n",
    "    daily_ice_max[i,:,:,:]   = daily_ice_oneens.max('dayofyear')\n",
    "    ice_thick_ann[i,:,:,:]   = daily_ice_oneens.mean('dayofyear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we have shifted the time index by -232 days, in order to get the correct day of year, we now shift it back\n",
    "'''\n",
    "iceon_mod       = iceon_mod.where(iceduration_mod > 5, nan) + 232\n",
    "iceoff_mod      = iceoff_mod.where(iceduration_mod > 5, nan)+ 232\n",
    "iceduration_mod = iceduration_mod.where(iceduration_mod >5,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceon_mod       = iceon_mod.transpose('ensemble','year','lat','lon')\n",
    "iceoff_mod      = iceoff_mod.transpose('ensemble','year','lat','lon')\n",
    "iceduration_mod = iceduration_mod.transpose('ensemble','year','lat','lon')\n",
    "daily_ice_max   = daily_ice_max.transpose('ensemble','year','lat','lon')\n",
    "ice_thick_ann   = ice_thick_ann.transpose('ensemble','year','lat','lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the Southern Hemisphere is different from the Northern Hemisphere, in which one ice cover season lies in one calendar year\n",
    "so the calculation is different from the Northern Hemisphere calculation.\n",
    "there is no shift in time index for the calculation in Southern Hemisphere  \n",
    "'''\n",
    "## calculate southern Hemisphere\n",
    "daily_ice_SH = ice_ds.LAKEICETHICK.sel(lat = slice (-64, -16), lon = slice(276,304), time = slice('1850-01-01','2099-12-31'))\n",
    "iceon_mod_SH = xr.DataArray(nan, \n",
    "                     attrs=daily_ice_SH.attrs,\n",
    "                     name='LAKEICETHICK',\n",
    "                     dims=('ensemble','lat','lon','year'), \n",
    "                     coords = {'ensemble':daily_ice_SH.ensemble,'lat':daily_ice_SH.lat,'lon':daily_ice_SH.lon,'year':np.arange(1850,2100)})\n",
    "iceoff_mod_SH = xr.DataArray(nan, \n",
    "                     attrs=daily_ice_SH.attrs,\n",
    "                     name='LAKEICETHICK',\n",
    "                     dims=('ensemble','lat','lon','year'), \n",
    "                     coords = {'ensemble':daily_ice_SH.ensemble,'lat':daily_ice_SH.lat,'lon':daily_ice_SH.lon,'year':np.arange(1850,2100)})\n",
    "daily_ice_SH_res = daily_ice_SH.assign_coords(time = ind).unstack('time')\n",
    "for i in tqdm(np.arange(90), desc = '1st loop'):\n",
    "    daily_ice_oneens = daily_ice_SH_res[i,:,:,:,:]\n",
    "    dayofyear_mask = daily_ice_oneens.dayofyear.where(daily_ice_oneens >0)\n",
    "    iceon_mod_SH[i,:,:,:]       = dayofyear_mask.min('dayofyear')\n",
    "    iceoff_mod_SH[i,:,:,:]      = dayofyear_mask.max('dayofyear')\n",
    "iceon_mod_SH       = iceon_mod_SH.where(iceon_mod_SH > 5, nan) \n",
    "iceoff_mod_SH      = iceoff_mod_SH.where(iceoff_mod_SH > 5, nan)\n",
    "iceon_mod_SH       = iceon_mod_SH.transpose('ensemble','year','lat','lon')\n",
    "iceoff_mod_SH      = iceoff_mod_SH.transpose('ensemble','year','lat','lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put the northern Hemisphere and southern Hemsiphere together\n",
    "iceon_mod[:,:,28:79,221:244] = iceon_mod_SH\n",
    "iceoff_mod[:,:,28:79,221:244] = iceoff_mod_SH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write them into nc files\n",
    "iceduration_mod.rename('iceduration').to_netcdf('/proj/lhuang/LENS/ICEDURATION_mod.nc')\n",
    "iceon_mod.rename('iceon').to_netcdf('/proj/lhuang/LENS/ICEON_mod.nc')\n",
    "iceoff_mod.rename('iceoff').to_netcdf('/proj/lhuang/LENS/ICEOFF_mod.nc')\n",
    "daily_ice_max.rename('icemax').to_netcdf('/proj/lhuang/LENS/ICEMAX_mod.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyncl] *",
   "language": "python",
   "name": "conda-env-pyncl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "6484171234cdd38788da5cb2d28a85e0dc377a9d1bd3aff49df05eb5ce254897"
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
